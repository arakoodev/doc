"use strict";(self.webpackChunkalekhaweb=self.webpackChunkalekhaweb||[]).push([[1477],{4556:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"Article-writer","metadata":{"permalink":"/blog/Article-writer","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-09-16-article-writer/index.md","source":"@site/blog/2023-09-16-article-writer/index.md","title":"How to Build a Article Writer Prompt with EdgeChains","description":"Generates an article for you using OpenAI API in Java and EdgeChain.","date":"2023-09-16T00:00:00.000Z","formattedDate":"September 16, 2023","tags":[{"label":"Article Writer","permalink":"/blog/tags/article-writer"},{"label":"Arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":8.62,"hasTruncateMarker":false,"authors":[{"name":"Aditya Pandey","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"Article-writer","title":"How to Build a Article Writer Prompt with EdgeChains","authors":{"name":"Aditya Pandey","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["Article Writer","Arakoo"]},"nextItem":{"title":"How to Build Semantic Search using EdgeChains","permalink":"/blog/Semantic-Search"}},"content":"**Generates an article for you using OpenAI API in Java and EdgeChain.**\\n\\nConsider you have to write an article about Global Warming:\\nNormally you have to find information about it in various websites, but this particular model helps you in generating the article by just providing the title of the article.\\nEdgeChain is a streamlined solution for developing GenAI applications, offering simplicity through a single script file and jsonnet file setup. It emphasizes versioning for prompts, automatic parallelism across various processors, fault tolerance, and scalability, making it a robust choice for chain-of-thought applications with extensive API integration and data sets. While LangChain primarily focuses on a specific set of principles, EdgeChain takes a unique stance, emphasizing declarative prompt and chain orchestration as pivotal components of its architecture. To delve deeper into EdgeChain and explore its capabilities, you can refer to GitHub repository [https://github.com/arakoodev/edgechains#why-do-you-need-declarative-prompt--chain-orchestration-](https://github.com/arakoodev/edgechains#why-do-you-need-declarative-prompt--chain-orchestration-). This platform offers a comprehensive view of EdgeChains\' vision and how it differentiates itself from LangChain.\\n\\n## Pre Requisites\\n\\n1. You need to make an account in OpenAI, Postgres so that from there you can retrieve the AUTH key, org id and etc. which are needed for the code.\\n![OpenAI Landing Page](./OpenAi.png)\\n2.  You need to download the edgechains jar file from this url https://github.com/arakoodev/EdgeChains/releases.\\n3. Download the .java and .jsonnet file and put them in the same folder.  \\n4. In the code according to the folder structure you have to write about the path.\\n \\n\\n## Configuration of the Database\\n\\n1.  Go to the Supabase website ([https://supabase.io](https://supabase.io)) and sign up for an account.\\n2.  Create a new Project by clicking the \u201cNew Project\u201d button.\\n3. Configure your project settings including the project name, region, and the plan.\\n4.  Once your project is created, you\u2019ll be directed to the project dashboard.\\n5. Click the \u201cCreate Database\u201d button to create a new PostgreSQL database.\\n6. After the database is created, you can access its credentials, including the database URL, API URL and service role key.\\n![Supabase Landing Page](./Supabase.png)\\n\\n## Explanation of the Code \\n\\n -  Load the edgechains package.  \\n - Import the OPENAI_Chat_Completion API. Here we have to import the static constants from other classes. These classes are  of OpenAI.\\n - Import the Spring Framework related classes and annotations.  \\n -   The code relies on external libraries and dependencies, such as  `com.edgechain.lib`  and  `io.reactivex.rxjava3`.These dependencies provide additional functionality and utilities for the code.\\n - Classes such as   `OpenAiEndpoint`,  `WikiEndpoint`,  `ArkRequest`, and  `CompletionRequest` are used to interact with specific endpoints or APIs, such as OpenAI and Wikipedia.\\n - RxJava and Retry Logic: The code uses RxJava and includes classes like  `ExponentialDelay`  and  `EdgeChain`.These are used for implementing retry logic and handling asynchronous operations.\\n - The code includes a constant  `OPENAI_CHAT_COMPLETION_API`, which represents the endpoint for OpenAI chat completion.\\n - A class named  `WikiExample`  is present that includes several static variables and a  `JsonnetLoader`  instance. Here\'s an explanation of the that:\\n\\n - Static Variables:\\n\\n    -   `OPENAI_AUTH_KEY`: This variable represents the OpenAI authentication key. It is a string that should be replaced with your actual OpenAI authentication key.\\n    -   `OPENAI_ORG_ID`: This variable represents the OpenAI organization ID. It is a string that should be replaced with your actual OpenAI organization ID.\\n    -   `gpt3Endpoint`: This variable is an instance of the  `OpenAiEndpoint`  class, which is used to communicate with OpenAI services.\\n    -   `gpt3StreamEndpoint`: This variable is another instance of the  `OpenAiEndpoint`  class, which is likely used for streaming communication with OpenAI services.\\n    -   `wikiEndpoint`: This variable is an instance of the  `WikiEndpoint`  class, which is used to communicate with the Wikipedia API.\\n - JsonnetLoader\\n\\n\\n    -   `loader`: This variable is an instance of the  `JsonnetLoader`  class, which is used to load and process Jsonnet files.\\n    -   `FileJsonnetLoader`: This class is a specific implementation of the  `JsonnetLoader`  interface that loads Jsonnet files from the file system.\\n    -   The  `FileJsonnetLoader`  constructor takes three arguments:\\n        -   The first argument represents the probability (in percentage) of executing the first file (`./wiki1.jsonnet`). In this case, there is a 70% chance of executing  `./wiki1.jsonnet`.\\n        -   The second argument is the path to the first Jsonnet file (`./wiki1.jsonnet`).\\n        -   The third argument is the path to the second Jsonnet file (`./wiki2.jsonnet`).\\n\\nThe purpose of this code is to create an instance of  `FileJsonnetLoader`  that loads Jsonnet files with a certain probability. Depending on the probability, either  `./wiki1.jsonnet`  or  `./wiki2.jsonnet`  will be executed.\\n![Main Method Page](./Main.png)\\n - The main method is the entry point of the application.\\n -      Setting Server Port:\\n    -   `System.setProperty(\\"server.port\\", \\"8080\\")`: This line sets the server port to 8080. It configures the application to listen on port 8080 for incoming requests.\\n\\n -     Configuring Properties:\\n    -   `Properties properties = new Properties()`: This line creates a new instance of the  `Properties`  class, which is used to store key-value pairs of configuration properties.\\n    -   `properties.setProperty(\\"cors.origins\\", \\"http://localhost:4200\\")`: This line sets the CORS (Cross-Origin Resource Sharing) origins property to allow requests from  `http://localhost:4200`. CORS is used to control access to resources from different origins.\\n\\n -     Configuring JPA and Hibernate Properties:\\n    -   `properties.setProperty(\\"spring.jpa.show-sql\\", \\"true\\")`: This line sets the property to show SQL queries executed by JPA (Java Persistence API).\\n    -   `properties.setProperty(\\"spring.jpa.properties.hibernate.format_sql\\", \\"true\\")`: This line sets the property to format the SQL queries executed by Hibernate.\\n\\n -     Configuring PostgreSQL Database Properties:\\n    -   `properties.setProperty(\\"postgres.db.host\\", \\"jdbc:postgresql://db.rkkbllhnexkzjyxhgexm.supabase.co:5432/postgres\\")`: This line sets the PostgreSQL database host URL.\\n    -   `properties.setProperty(\\"postgres.db.username\\", \\"postgres\\")`: This line sets the username for the PostgreSQL database.\\n    -   `properties.setProperty(\\"postgres.db.password\\", \\"jtGhg7?JLhUF$fK\\")`: This line sets the password for the PostgreSQL database.\\n\\n -     Starting the Spring Boot Application:\\n    -   `new SpringApplicationBuilder(WikiExample.class).properties(properties).run(args)`: This line creates a new instance of  `SpringApplicationBuilder`  with the  `WikiExample`  class as the main application class. It sets the configured properties and runs the Spring Boot application.\\n\\n -     Initializing Endpoints:\\n    -   `wikiEndpoint = new WikiEndpoint()`: This line creates an instance of the  `WikiEndpoint`  class, which is used to communicate with the Wikipedia API.\\n    -   `gpt3Endpoint = new OpenAiEndpoint(...)`: This line creates an instance of the  `OpenAiEndpoint`  class, which is used to communicate with OpenAI services. It sets various parameters such as the OpenAI chat completion API, authentication key, organization ID, model, temperature, and delay.\\n    -   `gpt3StreamEndpoint = new OpenAiEndpoint(...)`: This line creates another instance of the  `OpenAiEndpoint`  class, which is likely used for streaming communication with OpenAI services. It sets similar parameters as the  `gpt3Endpoint`, but with an additional flag for streaming.\\n![Endpoints](./Endpoint.png)\\n## Article Writer Controller\\n\\n - It is a `RestController`  class named  `ArticleController`  that handles HTTP GET requests for the  `/article`  endpoint. Here\'s an explanation of the code within the class:\\n\\n1.  `@RestController`  Annotation:\\n    -   This annotation is used to indicate that the class is a REST controller, which means it handles HTTP requests and returns the response in a RESTful manner.\\n\\n2.  `@GetMapping(\\"/article\\")`  Annotation:\\n    -   This annotation is used to map the HTTP GET requests with the  `/article`  endpoint to the  `generateArticle`  method.\\n\\n3.  `generateArticle`  Method:\\n    -   This method is responsible for generating an article based on the provided query parameter.\\n    -   It takes an  `ArkRequest`  object as a parameter, which is likely a custom request object that contains query parameters.\\n    -   The method throws an  `Exception`  if any error occurs during the generation process.\\n\\n4.  Generating the Prompt:\\n    -   The method prepares a prompt for the article generation by concatenating the string \\"Write an article about \\" with the value of the  `title`  query parameter from the  `arkRequest`  object.\\n\\n5.  Sending a Request to the OpenAI API:\\n    -   The method uses the  `gpt3Endpoint`  instance (which is an instance of the  `OpenAiEndpoint`  class) to send a request to the OpenAI API for generating the article.\\n    -   It uses the  `chatCompletion`  method of the  `gpt3Endpoint`  to perform the chat completion.\\n    -   The  `chatCompletion`  method takes the prompt, a chat model name (\\"React-Chain\\"), and the  `arkRequest`  object as parameters.\\n    -   The generated article is stored in the  `gptre`  variable.\\n\\n6.  Returning the Generated Article:\\n    -   The method returns the generated article as a response to the HTTP GET request.\\n\\n## Postman Testing\\nAfter all this we will be using the postman to test and give the requests for the same in the following manner:\\n**Title**\\n* Description: Perform a query to retrieve results from Open AI.\\n*  Method: POST \\n* URL:[localhost:8080/article?title=Global warming](http://localhost:8080/pinecone/upsert?namespace=machine-learning) \\n* Headers: Content-Type: application/json\\n* Body: raw\\n\\n\\n![Postman Page](./Postman.png)\\n### Full Working Code\\n\\n```java\\npackage com.edgechain;\\n\\nimport com.edgechain.lib.endpoint.impl.OpenAiEndpoint;\\nimport com.edgechain.lib.endpoint.impl.WikiEndpoint;\\nimport com.edgechain.lib.jsonnet.JsonnetArgs;\\nimport com.edgechain.lib.jsonnet.JsonnetLoader;\\nimport com.edgechain.lib.jsonnet.enums.DataType;\\nimport com.edgechain.lib.jsonnet.impl.FileJsonnetLoader;\\nimport com.edgechain.lib.openai.response.ChatCompletionResponse;\\nimport com.edgechain.lib.request.ArkRequest;\\nimport com.edgechain.lib.response.ArkResponse;\\nimport com.edgechain.lib.rxjava.retry.impl.ExponentialDelay;\\nimport com.edgechain.lib.rxjava.transformer.observable.EdgeChain;\\nimport com.edgechain.lib.openai.request.CompletionRequest;\\n\\n\\nimport io.reactivex.rxjava3.core.Observable;\\nimport java.util.*;\\nimport java.util.concurrent.TimeUnit;\\n\\nimport com.edgechain.lib.wiki.response.WikiResponse;\\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\\n\\nimport org.springframework.boot.builder.SpringApplicationBuilder;\\nimport org.springframework.web.bind.annotation.GetMapping;\\nimport org.springframework.web.bind.annotation.RestController;\\nimport org.springframework.web.bind.annotation.RequestParam;\\n\\nimport static com.edgechain.lib.constants.EndpointConstants.OPENAI_CHAT_COMPLETION_API;\\n\\n@SpringBootApplication\\npublic class WikiExample {\\n\\n  private static final String OPENAI_AUTH_KEY = \\"\\"; // YOUR OPENAI AUTH KEY\\n  private static final String OPENAI_ORG_ID = \\"\\"; // YOUR OPENAI ORG ID\\n\\n  /* Step 3: Create OpenAiEndpoint to communicate with OpenAiServices; */\\n  private static OpenAiEndpoint gpt3Endpoint;\\n\\n  private static OpenAiEndpoint gpt3StreamEndpoint;\\n\\n  private static WikiEndpoint wikiEndpoint;\\n\\n  // There is a 70% chance that file1 is executed; 30% chance file2 is executed....\\n  private final JsonnetLoader loader =\\n      new FileJsonnetLoader(70, \\"./wiki1.jsonnet\\", \\"./wiki2.jsonnet\\");\\n\\n  public static void main(String[] args) {\\n    System.setProperty(\\"server.port\\", \\"8080\\");\\n\\n    // Optional, for logging SQL queries (shouldn\'t be used in prod)\\n    Properties properties = new Properties();\\n\\n    // Adding Cors ==> You can configure multiple cors w.r.t your urls.;\\n    properties.setProperty(\\"cors.origins\\", \\"http://localhost:4200\\");\\n\\n    properties.setProperty(\\"spring.jpa.show-sql\\", \\"true\\");\\n    properties.setProperty(\\"spring.jpa.properties.hibernate.format_sql\\", \\"true\\");\\n\\n    properties.setProperty(\\"postgres.db.host\\", \\"\\");\\n    properties.setProperty(\\"postgres.db.username\\", \\"\\");\\n    properties.setProperty(\\"postgres.db.password\\", \\"\\");\\n\\n    new SpringApplicationBuilder(WikiExample.class).properties(properties).run(args);\\n\\n    wikiEndpoint = new WikiEndpoint();\\n\\n    gpt3Endpoint =\\n        new OpenAiEndpoint(\\n            OPENAI_CHAT_COMPLETION_API,\\n            OPENAI_AUTH_KEY,\\n            OPENAI_ORG_ID,\\n            \\"gpt-3.5-turbo\\",\\n            \\"user\\",\\n            0.7,\\n            new ExponentialDelay(3, 5, 2, TimeUnit.SECONDS));\\n\\n    gpt3StreamEndpoint =\\n        new OpenAiEndpoint(\\n            OPENAI_CHAT_COMPLETION_API,\\n            OPENAI_AUTH_KEY,\\n            OPENAI_ORG_ID,\\n            \\"gpt-3.5-turbo\\",\\n            \\"user\\",\\n            0.7,\\n            true,\\n            new ExponentialDelay(3, 5, 2, TimeUnit.SECONDS));\\n  }\\n  @RestController\\n  public class ArticleController {\\n\\t  @GetMapping(\\"/article\\")\\n      public String generateArticle(ArkRequest arkRequest) throws Exception {\\n          // Prepare the prompt\\n          String prompt = \\"Write an article about \\" + arkRequest.getQueryParam(\\"title\\") + \\".\\";\\n\\n          // Use the OpenAiService to send a request to the OpenAI API\\n          // GPT-3 model is used with the provided prompt, max tokens is set to 500 for the article length\\n          // and temperature is set to 0.7 which is a good balance between randomness and consistency\\n          // Echo is set to true to include the prompt in the response\\n          /*CompletionRequest completionRequest = CompletionRequest.builder()\\n                  .prompt(prompt)\\n                  .model(\\"gpt-3.5-turbo\\")\\n                  .maxTokens(500)\\n                  .temperature(0.7)\\n                  //.echo(true)\\n                  .build();*/\\n          String gptre=new EdgeChain<>(gpt3Endpoint.chatCompletion(prompt, \\"React-Chain\\", arkRequest))\\n                  .get()\\n                  .getChoices()\\n                  .get(0)\\n                  .getMessage()\\n                  .getContent();\\n          // Send the request\\n          //ChatCompletionResponse response = gpt3Endpoint.chat(completionRequest);\\n          //Observable<ChatCompletionResponse> response = gpt3Endpoint.chatCompletion(completionRequest.getPrompt(), \\"\\", arkRequest);\\n\\n          // Extract the generated text from the response\\n          //String generatedArticle = response.getChoices().get(0).getGeneratedText();\\n          //return response.blockingFirst().getChoices().get(0).toString();\\n          return gptre;\\n\\n          // Return the generated article\\n//          return generatedArticle;\\n      }\\n  }\\n}\\n```"},{"id":"Semantic-Search","metadata":{"permalink":"/blog/Semantic-Search","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-09-16-semantic-search/index.md","source":"@site/blog/2023-09-16-semantic-search/index.md","title":"How to Build Semantic Search using EdgeChains","description":"Search for your document from a large pack of existing documents in the most effective manner.","date":"2023-09-16T00:00:00.000Z","formattedDate":"September 16, 2023","tags":[{"label":"Semantic Search","permalink":"/blog/tags/semantic-search"},{"label":"Arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":10.27,"hasTruncateMarker":false,"authors":[{"name":"Aditya Pandey","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"Semantic-Search","title":"How to Build Semantic Search using EdgeChains","authors":{"name":"Aditya Pandey","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["Semantic Search","Arakoo"]},"prevItem":{"title":"How to Build a Article Writer Prompt with EdgeChains","permalink":"/blog/Article-writer"},"nextItem":{"title":"Getting Started with Supabase","permalink":"/blog/Supabase"}},"content":"Search for your document from a large pack of existing documents in the most effective manner.\\n\\nConsider you have a notice of opening of campus of some Institute :  \\nYou want to find a particular campus that is similar to some campuses of Kolkata. The best effective way to do so would be by using the Semantic Search.\\n\\nSemantic Search is the task of retrieving the documents from a collection of the documents in response to a query asked .It allows you to access the best matches from your document collection within seconds and the best thing is the fact is on the basis of meaning rather than keyword matches .A Semantic Search model has been created using Java.  \\n[Edgechains](https://github.com/arakoodev/edgechains) is a streamlined solution for developing GenAI applications, offering simplicity through a single script file and jsonnet file setup. It emphasizes versioning for prompts, automatic parallelism across various processors, fault tolerance, and scalability, making it a robust choice for chain-of-thought applications with extensive API integration and data sets. While Langchain primarily focuses on a specific set of principles, EdgeChains takes a unique stance, emphasizing declarative prompt and chain orchestration as pivotal components of its architecture. To delve deeper into EdgeChains and explore its capabilities, you can refer to our [GitHub repository](https://github.com/arakoodev/edgechains#why-do-you-need-declarative-prompt--chain-orchestration-) . \\n### Pre Requisites\\n\\n1.  You need to make an account in OPENAI, Pinecone, Postgres so that from there you can retrieve the auth key, org id and etc which are needed for the code.  \\n2.  You need to download the edgechains jar file from this url https://github.com/arakoodev/EdgeChains/releases.  \\n3. Download the `.java` and `.jsonnet` file and put them in the same folder.  \\n4. In the code according to the folder structure you have to write about the path.  \\n  \\n  \\n\\n### Configuration of the Database:\\n\\n1.  Go to the Supabase website ([https://supabase.io](https://supabase.io)) and sign up for an account.\\n    \\n2.  Create a new Project by clicking the \u201cNew Project\u201d button.\\n    \\n3.  Configure your project settings including the project name, region, and the plan.\\n    \\n4.  Once your project is created, you\u2019ll be directed to the project dashboard.\\n    \\n5.  Click the \u201cCreate Database\u201d button to create a new PostgreSQL database.\\n    \\n6.  After the database is created, you can access its credentials, including the database URL, API URL and service role key.  \\n    \\n\\n### Explanation of the Code\\n\\n1. Load the edgechains package.  \\n2. Import the OPENAI_Chat_Completion API. Here we have to import the static constants from other classes. These classes are Pinecone, OpenAI, and PDF reading.  \\n3. Import the Spring Framework related classes and annotations.  \\n![Spring Page](./Spring.png)\\n4. Fill in the details of the `auth key`, `org_id` and others of OpenAI and Pinecone.  \\n![OpenAI Page](./OpenAi.png)\\n5. Create variables that may be interacting with the OpenAI services used to store authentication keys and API endpoints.  \\n6. Load the jsonnet file into the variable, and then load the data of that file into the variable.  \\n7. In the main method, set the system server port to the desired port or by default it is `8080`. `Properties properties = new Properties ();` this property is often used in Java to manage the key-value pairs.  \\n8. Then you have to configure the Hibernate to format the SQL queries for better readability. The lines written below are used to access the database. They enable SQL query logging and formatting in Spring JPA (Java Persistence API).\\n![CORS Page](./springjpa.png)\\n\\n9. Set the CORS (Cross-Origin Resource Sharing) by specifying the allowed origins.  \\n10. Then you have to initialize several endpoints and related configurations.  \\n* `ada002Embedding`: This variable is an instance of the `OpenAiEndpoint` class, which is used to interact with OpenAI for text embeddings.  It is a configuration object that allows your Java code to communicate with OpenAI\'s text embedding service using the ***\\"Ada 002\\"*** model and handle authentication and retry logic. It\'s a part of integrating OpenAI\'s capabilities into your application for NLP tasks.  \\n* `gpt3Endpoint`: This variable is another instance of the OpenAiEndpoint class, configured for GPT-3.5 Turbo, a language model for chat completions. It has similar configuration parameters as ada002Embedding but with additional settings for chat completions.  It is a configuration object that facilitates communication with OpenAI\'s ***\\"GPT-3.5 Turbo\\"*** model for chat-based language generation tasks. \\n*  `upsertPineconeEndpoint`: This variable is an instance of the `PineconeEndpoint` class, which is used to interact with Pinecone for upserting vectors. It is a configuration object that enables your application to interact with the Pinecone service for adding or updating vectors in a vector index. It plays a crucial role in enabling similarity-based search and retrieval functionality in your application.  \\nThese variables are essential for communicating with external services such as OpenAI and Pinecone.  \\n![Endpoint Page](./Endpoints.png)\\n11. After this, define a Spring class which is responsible for handling the HTTP requests and contains various methods for interacting with Pinecone and OpenAI services.\\n12. Inject a `PdfReader` instance into the controller so that it can be used to read PDF files in the code. \\n \\n### Upsert Controller  \\nCreate a method `upsertPinecone` that handles HTTP POST requests to the endpoint which takes an `ArkRequest` object as a parameter which contains data required for the operation. Here `pdfReader` is used to read the input stream in chunks of 512 bytes and stores the result in a string array. An instance of `PineconeRetrieval` is created passing the string array created, the `ada002Embedding` endpoint, the `upsertPineconeEndpoint` and the `arkRequest`. The `upsert` method of the `PineconeRetrieval` instance is called to upsert the data into Pinecone. The pdf that you uploaded is divided into chunks and then the embedding vector sends that to the embedding endpoint to perform the similarity search on the query sent. \\n**NOTE**: Upsert is done only once as you can upload as many pdf files one at a time as after upserting the major work you would be doing is querying.  \\n![Upsert Page](./Upsert.png)\\n### Query Controller\\nDefine a method \u2018query\u2019 to handle HTTP POST requests to the `/pinecone/query` endpoint. It extracts the various parameters such as `namespace`, `query` from the `arkRequest` object. Here EdgeChain performs a Pinecone query using the `queryPineconeEndpoint` and retrieves word embeddings. This helps in transforming the results of the query into a list of the objects using a method. It returns the response containing the result of the query. `topk` is used to divide the pdf into chunks.  \\n![Query Page](./query.png)  \\nOverall this model manages the interaction with Pinecone and OpenAI services including upserting data, querying based on the received HTTP requests.\\n\\n  \\n\\n### Postman Testing\\n\\nAfter all this we will be using the postman to test and give the requests for the same in the following manner:  \\n1. **Upsert**\\n   *    Description: Upload a file to update Open AI data in the Pinecone namespace.\\n    \\n   *  Method: POST\\n    \\n    *  URL:[http://localhost:8080/pinecone/upsert?namespace=machine-learning](http://localhost:8080/pinecone/upsert?namespace=machine-learning)\\n    \\n   *   Headers: Content-Type: multipart/form-data\\n    \\n   *   Body:  \\n       * Mode: formdata  \\n        * Key: file  \\n        * Type: file\\n    \\n\\nAlso one of the important to consider while uploading the files, upload only in pdf format, json format is not allowed. You can only upload one pdf file at a time.\\n\\n\\n2. **Query**\\n\\n   -   Description: Perform a query to retrieve results from Open AI in the Pinecone namespace.\\n    \\n   -   Method: POST\\n    \\n   -   URL:http://localhost:8080/pinecone/query?topK=6&namespace=machine-learning\\n    \\n   -   Headers: Content-Type: application/json  \\n      \\n    \\n    -   Body:  \\n       * Mode: raw  \\n      *   Data:  `{\\"query\\": \\"When is the Kolkata campus opening?\\"  }`\\n\\n![Postman Page](./postmantesting.png)\\n\\nSemantic Search can change the way searching, ranking or retrieval systems work with their ability to index and store embedding vectors. Here EdgeChain helps in inserting the chunk and calculating the embedding vector. It provides methods which make it very easy for the customer so that they have just to write the business logic and their work happens easily, instead EdgeChain does everything. It ranks according to the score where the score is according to how close their similarity is with the query. And the model will return the answer of the query according to the ranking.\\n\\nThe Full Working Code of the Model is.  \\n\\n```java\\npackage com.edgechain;\\n\\n  \\n\\nimport  static com.edgechain.lib.constants.EndpointConstants.OPENAI_CHAT_COMPLETION_API;\\n\\nimport  static com.edgechain.lib.constants.EndpointConstants.OPENAI_EMBEDDINGS_API;\\n\\n  \\n\\nimport com.edgechain.lib.chains.PineconeRetrieval;\\n\\nimport com.edgechain.lib.context.domain.HistoryContext;\\n\\nimport com.edgechain.lib.embeddings.WordEmbeddings;\\n\\nimport com.edgechain.lib.endpoint.impl.OpenAiEndpoint;\\n\\nimport com.edgechain.lib.endpoint.impl.PineconeEndpoint;\\n\\nimport com.edgechain.lib.endpoint.impl.RedisHistoryContextEndpoint;\\n\\nimport com.edgechain.lib.jsonnet.JsonnetArgs;\\n\\nimport com.edgechain.lib.jsonnet.JsonnetLoader;\\n\\nimport com.edgechain.lib.jsonnet.enums.DataType;\\n\\nimport com.edgechain.lib.jsonnet.impl.FileJsonnetLoader;\\n\\nimport com.edgechain.lib.openai.response.ChatCompletionResponse;\\n\\nimport com.edgechain.lib.reader.impl.PdfReader;\\n\\nimport com.edgechain.lib.request.ArkRequest;\\n\\nimport com.edgechain.lib.response.ArkResponse;\\n\\nimport com.edgechain.lib.rxjava.retry.impl.ExponentialDelay;\\n\\nimport com.edgechain.lib.rxjava.retry.impl.FixedDelay;\\n\\nimport com.edgechain.lib.rxjava.transformer.observable.EdgeChain;\\n\\nimport java.io.IOException;\\n\\nimport java.io.InputStream;\\n\\nimport java.util.*;\\n\\nimport java.util.concurrent.TimeUnit;\\n\\nimport org.springframework.beans.factory.annotation.Autowired;\\n\\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\\n\\nimport org.springframework.boot.builder.SpringApplicationBuilder;\\n\\nimport org.springframework.web.bind.annotation.*;\\n\\n  \\n\\n@SpringBootApplication\\n\\npublic  class PineconeExample {\\n\\n  \\n\\nprivate  static  final String OPENAI_AUTH_KEY = \\"\\"; // YOUR OPENAI AUTH KEY\\n\\nprivate  static  final String OPENAI_ORG_ID = \\"\\"; // YOUR OPENAI ORG ID\\n\\nprivate  static  final String PINECONE_AUTH_KEY = \\"\\";\\n\\nprivate  static  final String PINECONE_QUERY_API = \\"\\";\\n\\nprivate  static  final String PINECONE_UPSERT_API = \\"\\";\\n\\nprivate  static  final String PINECONE_DELETE = \\"\\";\\n\\n  \\n\\nprivate  static OpenAiEndpoint ada002Embedding;\\n\\nprivate  static OpenAiEndpoint gpt3Endpoint;\\n\\n  \\n\\nprivate  static PineconeEndpoint upsertPineconeEndpoint;\\n\\nprivate  static PineconeEndpoint queryPineconeEndpoint;\\n\\n  \\n\\nprivate  static PineconeEndpoint deletePineconeEndpoint;\\n\\n  \\n\\nprivate  static RedisHistoryContextEndpoint contextEndpoint;\\n\\n  \\n\\nprivate JsonnetLoader queryLoader = new FileJsonnetLoader(\\"./pinecone-query.jsonnet\\");\\n\\nprivate JsonnetLoader chatLoader = new FileJsonnetLoader(\\"./pinecone-chat.jsonnet\\");\\n\\n  \\n\\npublic  static  void main(String[] args) {\\n\\nSystem.setProperty(\\"server.port\\", \\"8080\\");\\n\\nProperties properties = new Properties();\\n\\n  \\n\\nproperties.setProperty(\\"spring.jpa.show-sql\\", \\"true\\");\\n\\nproperties.setProperty(\\"spring.jpa.properties.hibernate.format_sql\\", \\"true\\");\\n\\n  \\n\\n// Adding Cors ==> You can configure multiple cors w.r.t your urls.;\\n\\nproperties.setProperty(\\"cors.origins\\", \\"http://localhost:4200\\");\\n\\n  \\n\\n// Redis Configuration\\n\\n/*properties.setProperty(\\"redis.url\\", \\"\\");\\n\\nproperties.setProperty(\\"redis.port\\", \\"\\");\\n\\nproperties.setProperty(\\"redis.username\\", \\"default\\");\\n\\nproperties.setProperty(\\"redis.password\\", \\"\\");\\n\\nproperties.setProperty(\\"redis.ttl\\", \\"3600\\");*/\\n\\n  \\n\\n// If you want to use PostgreSQL only; then just provide dbHost, dbUsername & dbPassword.\\n\\n// If you haven\'t specified PostgreSQL, then logs won\'t be stored.\\n\\nproperties.setProperty(\\"postgres.db.host\\", \\"\\");\\n\\nproperties.setProperty(\\"postgres.db.username\\", \\"\\");\\n\\nproperties.setProperty(\\"postgres.db.password\\", \\"\\");\\n\\n  \\n\\nnew SpringApplicationBuilder(PineconeExample.class).properties(properties).run(args);\\n\\n  \\n\\n// Variables Initialization ==> Endpoints must be intialized in main method...\\n\\nada002Embedding =\\n\\nnew OpenAiEndpoint(\\n\\nOPENAI_EMBEDDINGS_API,\\n\\nOPENAI_AUTH_KEY,\\n\\nOPENAI_ORG_ID,\\n\\n\\"text-embedding-ada-002\\",\\n\\nnew ExponentialDelay(3, 3, 2, TimeUnit.SECONDS));\\n\\n  \\n\\ngpt3Endpoint =\\n\\nnew OpenAiEndpoint(\\n\\nOPENAI_CHAT_COMPLETION_API,\\n\\nOPENAI_AUTH_KEY,\\n\\nOPENAI_ORG_ID,\\n\\n\\"gpt-3.5-turbo\\",\\n\\n\\"user\\",\\n\\n0.85,\\n\\nnew ExponentialDelay(3, 5, 2, TimeUnit.SECONDS));\\n\\n  \\n\\nupsertPineconeEndpoint =\\n\\nnew PineconeEndpoint(\\n\\nPINECONE_UPSERT_API,\\n\\nPINECONE_AUTH_KEY,\\n\\nnew ExponentialDelay(3, 3, 2, TimeUnit.SECONDS));\\n\\n  \\n\\nqueryPineconeEndpoint =\\n\\nnew PineconeEndpoint(\\n\\nPINECONE_QUERY_API, PINECONE_AUTH_KEY, new ExponentialDelay(3, 3, 2, TimeUnit.SECONDS));\\n\\n  \\n\\ndeletePineconeEndpoint =\\n\\nnew PineconeEndpoint(\\n\\nPINECONE_DELETE, PINECONE_AUTH_KEY, new FixedDelay(4, 5, TimeUnit.SECONDS));\\n\\n  \\n\\ncontextEndpoint =\\n\\nnew RedisHistoryContextEndpoint(new ExponentialDelay(2, 2, 2, TimeUnit.SECONDS));\\n\\n}\\n\\n  \\n\\n/**\\n\\n* By Default, every API is unauthenticated & exposed without any sort of authentication; To\\n\\n* authenticate, your custom APIs in Controller you would need @PreAuthorize(hasAuthority(\\"\\"));\\n\\n* this will authenticate by JWT having two fields: a) email, b) role:\\"authenticated,user_create\\"\\n\\n* To authenticate, internal APIs related to historyContext & Logging, Delete Redis/Postgres we\\n\\n* need to create bean of AuthFilter; you can uncomment the code. Note, you need to define\\n\\n* \\"jwt.secret\\" property as well to decode accessToken.\\n\\n*/\\n\\n// @Bean\\n\\n// @Primary\\n\\n// public AuthFilter authFilter() {\\n\\n// AuthFilter filter = new AuthFilter();\\n\\n// // ======== new MethodAuthentication(List.of(APIs), authorities) =============\\n\\n// filter.setRequestPost(new MethodAuthentication(List.of(\\"/v1/postgresql/historycontext\\"),\\n\\n// \\"authenticated\\")); // define multiple roles by comma\\n\\n// filter.setRequestGet(new MethodAuthentication(List.of(\\"\\"), \\"\\"));\\n\\n// filter.setRequestDelete(new MethodAuthentication(List.of(\\"\\"), \\"\\"));\\n\\n// filter.setRequestPatch(new MethodAuthentication(List.of(\\"\\"), \\"\\"));\\n\\n// filter.setRequestPut(new MethodAuthentication(List.of(\\"\\"), \\"\\"));\\n\\n// return filter;\\n\\n// }\\n\\n  \\n\\n@RestController\\n\\npublic  class PineconeController {\\n\\n  \\n\\n@Autowired  private PdfReader pdfReader;\\n\\n  \\n\\n/********************** PINECONE WITH OPENAI ****************************/\\n\\n  \\n\\n/**\\n\\n* Namespace: VectorDb allows you to partition the vectors in an index into namespaces. Queries\\n\\n* and other operations are then limited to one namespace, so different requests can search\\n\\n* different subsets of your index. If namespace is null or empty, in pinecone it will be\\n\\n* prefixed as \\"\\" empty string & in redis it will be prefixed as \\"knowledge\\" For example, you\\n\\n* might want to define a namespace for indexing books by finance, law, medicine etc.. Can be\\n\\n* used in multiple use-cases.... such as User uploading book, generating unique namespace &\\n\\n* then querying/chatting with it..\\n\\n*\\n\\n* @param arkRequest\\n\\n* @return\\n\\n*/\\n\\n// Namespace is optional (if not provided, it will be using Empty String \\"\\")\\n\\n@PostMapping(\\"/pinecone/upsert\\") // /v1/examples/openai/upsert?namespace=machine-learning\\n\\npublic  void upsertPinecone(ArkRequest arkRequest) throws IOException {\\n\\n  \\n\\n// String namespace = arkRequest.getQueryParam(\\"namespace\\");\\n\\nInputStream file = arkRequest.getMultiPart(\\"file\\").getInputStream();\\n\\n  \\n\\n// Configure Pinecone\\n\\n// upsertPineconeEndpoint.setNamespace(namespace);\\n\\n  \\n\\nString[] arr = pdfReader.readByChunkSize(file, 512);\\n\\n  \\n\\n/**\\n\\nRetrieval Class is basically used to generate embeddings & upsert it to VectorDB; If OpenAI\\n\\nEmbedding Endpoint is not provided; then Doc2Vec constructor is used If the model is not\\n\\nprovided, then it will emit an error*/\\n\\nPineconeRetrieval retrieval =\\n\\nnew PineconeRetrieval(arr, ada002Embedding, upsertPineconeEndpoint, arkRequest);\\n\\n  \\n\\nretrieval.upsert();\\n\\n}\\n\\n  \\n\\n@PostMapping(value = \\"/pinecone/query\\")\\n\\npublic ArkResponse query(ArkRequest arkRequest) {\\n\\n  \\n\\n//String namespace = arkRequest.getQueryParam(\\"namespace\\");\\n\\nString query = arkRequest.getBody().getString(\\"query\\");\\n\\nint  topK = arkRequest.getIntQueryParam(\\"topK\\");\\n\\n  \\n\\n// Configure Pinecone\\n\\n// queryPineconeEndpoint.setNamespace(namespace);\\n\\n  \\n\\n// Step 1: Chain ==> Get Embeddings From Input & Then Query To Pinecone\\n\\nEdgeChain<WordEmbeddings> embeddingsChain =\\n\\nnew EdgeChain<>(ada002Embedding.embeddings(query, arkRequest));\\n\\n  \\n\\n// Step 2: Chain ==> Query Embeddings from Pinecone\\n\\nEdgeChain<List<WordEmbeddings>> queryChain =\\n\\nnew EdgeChain<>(queryPineconeEndpoint.query(embeddingsChain.get(), topK));\\n\\n  \\n\\n// Chain 3 ===> Our queryFn passes takes list and passes each response with base prompt to\\n\\n// OpenAI\\n\\nEdgeChain<List<ChatCompletionResponse>> gpt3Chain =\\n\\nqueryChain.transform(wordEmbeddings -> queryFn(wordEmbeddings, arkRequest));\\n\\n  \\n\\nreturn  gpt3Chain.getArkResponse();\\n\\n}\\n```"},{"id":"Supabase","metadata":{"permalink":"/blog/Supabase","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-07-22-Supabase/index.md","source":"@site/blog/2023-07-22-Supabase/index.md","title":"Getting Started with Supabase","description":"Introduction","date":"2023-07-22T00:00:00.000Z","formattedDate":"July 22, 2023","tags":[{"label":"Supabase","permalink":"/blog/tags/supabase"},{"label":"Arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":2.92,"hasTruncateMarker":false,"authors":[{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"Supabase","title":"Getting Started with Supabase","authors":{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["Supabase","Arakoo"]},"prevItem":{"title":"How to Build Semantic Search using EdgeChains","permalink":"/blog/Semantic-Search"},"nextItem":{"title":"How to create an index in Pinecone","permalink":"/blog/Pinecone"}},"content":"## Introduction\\n**Supabase** is a powerful, open-source platform that simplifies the creation of secure and high-performance Postgres backends, offering functionalities similar to Firebase, such as authentication and real-time database. When used with **EdgeChains**, Supabase enhances the backend capabilities of applications built with the framework, enabling developers to create advanced and interactive applications powered by large language models.\\n\\n## Supabase Integration with EdgeChains\\n\\nIn the EdgeChains configuration, the following parameters need to be configured for Supabase integration:\\n\\n- **SupabaseURL**: The URL of the Supabase backend, which allows EdgeChains to communicate with the Supabase service.\\n\\n- **Supabase AnnonKey**: The anonymous key used for authentication when interacting with the Supabase backend.\\n\\n- **Supabase JWTSecret**: The JSON Web Token (JWT) secret for secure communication and user authentication.\\n\\n- **Supabase DBhost:**: The JDBC URL for connecting to the PostgreSQL database in Supabase. This URL provides the necessary information to establish a connection to the database. _ie, jdbc:postgresql://${SUPABASE_DB_URK}/postgres_\\n\\n- **DbUsername**: The username for the PostgreSQL database in Supabase. In this example, it is set to _postgres_.\\n\\n- **DbPassword**: The password for the PostgreSQL database in Supabase, which is required for authentication and access to the database.\\n\\nBy providing the appropriate values for these configuration parameters, EdgeChains can seamlessly integrate with Supabase, enabling developers to leverage the features and functionalities of Supabase as the backend for their language model-powered applications.\\n\\n---\\n## How to Get Configuration Parameters for Supabase Integration\\n\\nTo integrate Supabase with EdgeChains and obtain the necessary configuration parameters, follow these step-by-step instructions:\\n\\n**Step 1: Visit the Supabase website**\\n\\n- If you already have an account, click on the _Dashboard_ button in the top right corner and  and log in using your credentials.\\n- If you don\'t have an account, click on the _Sign in_ button to either log in or create a new account.\\n\\n![Pinecone landing page](./landing.png)\\n\\n- You can sign up using your email address or opt for a seamless registration process using your GitHub or SSO credentials.\\n\\n![Pinecone landing page](./signup.png)\\n\\n**Step 2: Create a New Project**\\n\\n- After logging in or signing up, you will be directed to the dashboard. Click on the _New project_ button to initiate the project creation process.\\n\\n![Pinecone landing page](./newproj.png)\\n\\n- Enter the necessary details for the database, including the Name, and set up a strong password for added security.\\n- Select your preferred region. For the free pricing plan, choose the default option.\\n\\n![Pinecone landing page](./createDB.png)\\n\\n- Finally, click on the _Create new project_ button to have Supabase set up your new project.\\n\\n**Step 3: Access Project Settings**\\n\\n- After your project is set up, go to the databases section and then proceed to the settings section of the database.\\n\\n![Pinecone landing page](./created.png)\\n\\n![Pinecone landing page](./DBsettings.png)\\n\\n- In this section, you will find the Host, User, and Password, which you need to take note of for using with EdgeChains. These parameters will facilitate the integration of Supabase with EdgeChains and enable seamless communication between the two platforms.\\n\\n**Step 4: Obtain API Credentials**\\n\\n- In the Supabase dashboard, navigate to the API section, where you can access the required URL and anonymous key.\\n\\n![Pinecone landing page](./API.png)\\n\\n- Continue scrolling down to find the JWT settings, where you can obtain the JWT secret as well.\\n\\n![Pinecone landing page](./JWT.png)\\n\\nBy following these steps and obtaining the necessary configuration parameters, you will successfully integrate Supabase with EdgeChains. These parameters will enable you to leverage Supabase\'s powerful features as the backend for your language model-powered applications, creating secure and high-performance Postgres backends with ease.\\n\\n---"},{"id":"Pinecone","metadata":{"permalink":"/blog/Pinecone","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-07-14-Pinecone/index.md","source":"@site/blog/2023-07-14-Pinecone/index.md","title":"How to create an index in Pinecone","description":"Introduction","date":"2023-07-14T00:00:00.000Z","formattedDate":"July 14, 2023","tags":[{"label":"Pinecone","permalink":"/blog/tags/pinecone"},{"label":"Arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":4.18,"hasTruncateMarker":false,"authors":[{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"Pinecone","title":"How to create an index in Pinecone","authors":{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["Pinecone","Arakoo"]},"prevItem":{"title":"Getting Started with Supabase","permalink":"/blog/Supabase"},"nextItem":{"title":"How to create a new instance in Redis","permalink":"/blog/redis"}},"content":"## Introduction\\nPinecone is a powerful vector database designed for efficient storage and querying of high-dimensional vectors. It provides a scalable and fast solution for applications involving similarity search, recommendation systems, natural language processing, and machine learning. With its simple API, advanced indexing algorithms, and real-time capabilities, Pinecone empowers developers to build high-performance applications that rely on vector-based data, delivering near-instantaneous search results and enabling personalized user experiences.\\n\\nPinecone seamlessly integrates with EdgeChains to enhance the performance of your language models. To get started, you\'ll need to obtain the URL from Pinecone and configure it in your EdgeChains application. Follow the steps below to achieve this.\\n\\n### Key Features\\nPinecone offers a range of key features and benefits that make it a powerful tool for working with high-dimensional vectors and enabling efficient similarity search. Some of its key features include:\\n\\n**Scalability:** Pinecone is designed to scale effortlessly, allowing you to handle massive amounts of data and millions to billions of vectors efficiently. It can handle high read and write throughput, making it suitable for demanding real-time applications.\\n\\n**High-Performance Search:** Pinecone leverages advanced indexing algorithms to provide fast and accurate similarity search. It enables efficient nearest neighbor search, allowing you to find the most similar vectors to a given query vector with sublinear time complexity.\\n\\n**Real-Time Updates:** With Pinecone, you can easily update your vector database in real-time, enabling you to handle dynamic data that changes frequently. This makes it ideal for applications that require continuous updates and real-time recommendations.\\n\\n**Flexible Vector Storage:** Pinecone provides flexible options for storing and representing vectors, supporting various data types and formats. It allows you to work with diverse types of data, including numerical embeddings, textual data, images, and more.\\n\\n**API and Query Language:** Pinecone offers a simple and intuitive API for managing and querying vector data. It provides a powerful query language that allows you to express complex similarity queries and filter results based on specific criteria.\\n\\n### Obtaining the Pinecone URL\\n\\nTo get started with Pinecone and obtain your Pinecone URL, follow these steps:\\n\\n**Step 1:** Visit the [Pinecone website](https://www.pinecone.io/).\\n\\nIf you already have an account, click on the _Login_ button in the top right corner and enter your credentials. Otherwise, click on the _Sign Up Free_ button to create a new account.\\n\\n![Pinecone landing page](./sign.png)\\n\\nYou can sign up using your email address alone, or choose to sign up with your Gmail, GitHub, or Microsoft account for a seamless registration process.\\n\\n![Pinecone sign up page](./signup.png)\\n\\n**Step 2:** Once you have logged in or signed up, you will be redirected to the dashboard where it may take a few moments to load your indexes. Please be patient during this process.\\n\\n**Step 3:**  After the indexes have finished loading, you can create a new index by clicking on the _Create Index_ button.\\n\\n![Create a new index](./create.png)\\n\\n- Provide a suitable name for your index. The name should only contain lowercase letters, numbers, and hyphens. It cannot be more than 45 characters long.\\n- Specify the dimension and metric for your index. The dimension refers to the length of the vectors you will be working with, and the metric determines the similarity measurement used for search operations. Choose the appropriate values for your use case.\\n- Once you have entered the necessary information, click on the _Create Index_ button to create your index.\\n\\n![Enter details](./details.png)\\n\\n**Step 4:** After the index is successfully created, you will be provided with your Pinecone URL. This URL represents the endpoint for accessing and interacting with your newly created index.\\n\\n![Pinecone vector DB URL](./URL.png)\\n\\nWith your Pinecone URL in hand, you can now integrate Pinecone into your applications and leverage its powerful vector similarity search capabilities.\\n\\n### Integration with EdgeChains\\n\\nTo seamlessly integrate Pinecone with EdgeChains, you can leverage the Pinecone URL obtained in the previous step and configure it in your EdgeChains application. This configuration enables your language models in EdgeChains to harness the powerful vector similarity search capabilities provided by Pinecone.\\n\\nTo achieve this integration, you will need to provide the following data in the `Starter class` of your `EdgeChainApplication.java` file:\\n\\n- **Query Endpoint:** This endpoint allows you to send queries to Pinecone for similarity search. The URL format is ${PINECONE_URL}/query.\\n\\n- **Upsert Endpoint:** Use this endpoint to insert or update vectors in your Pinecone index. The URL format is ${PINECONE_URL}/vectors/upsert.\\n\\n- **Delete Endpoint:** This endpoint enables you to remove vectors from your Pinecone index. The URL format is ${PINECONE_URL}/vectors/delete.\\n\\nFor example, if your Pinecone URL is `https://pinecone-sample-ccc3dd8.svc.asia-southeast-gcp-free.pinecone.io`, you would configure the following variables in the Starter class of your EdgeChainApplication.java file:\\n\\n``` bash\\n// YOUR PINECONE API KEY\\nprivate final String PINECONE_AUTH_KEY = \\n\\"https://pinecone-sample-ccc3dd8.svc.asia-southeast-gcp-free.pinecone.io\\"; \\n// YOUR PINECONE QUERY API\\nprivate final String PINECONE_QUERY_API = \\n\\"https://pinecone-sample-ccc3dd8.svc.asia-southeast-gcp-free.pinecone.io/query\\"; \\n// YOUR PINECONE UPSERT API\\nprivate final String PINECONE_UPSERT_API = \\n\\"https://pinecone-sample-ccc3dd8.svc.asia-southeast-gcp-free.pinecone.io/upsert\\"; \\n// YOUR PINECONE DELETE\\nprivate final String PINECONE_DELETE = \\n\\"https://pinecone-sample-ccc3dd8.svc.asia-southeast-gcp-free.pinecone.io/delete\\"; \\n``` \\n\\n### Additional Resources\\n\\nFor more information on Pinecone, tutorials, and community resources, you can refer to the following links:\\n\\n- [Pinecone Documentation](https://docs.pinecone.io/)\\n- [Pinecone Community Forum](https://www.pinecone.io/community/)"},{"id":"redis","metadata":{"permalink":"/blog/redis","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-07-14-Redis/index.md","source":"@site/blog/2023-07-14-Redis/index.md","title":"How to create a new instance in Redis","description":"I. Introduction","date":"2023-07-14T00:00:00.000Z","formattedDate":"July 14, 2023","tags":[{"label":"Redis","permalink":"/blog/tags/redis"},{"label":"Arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":2.345,"hasTruncateMarker":false,"authors":[{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"redis","title":"How to create a new instance in Redis","authors":{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["Redis","Arakoo"]},"prevItem":{"title":"How to create an index in Pinecone","permalink":"/blog/Pinecone"},"nextItem":{"title":"How to Craft a Stellar GitHub Support Bot with GPT-3 and Chain-of-Thought","permalink":"/blog/github-gpt"}},"content":"## I. Introduction\\nRedis is an open-source, in-memory data structure store that can be used as a database, cache, or message broker. It is known for its fast performance and versatility, making it a popular choice for various applications, including EdgeChains.\\n\\nOne of the key benefits of Redis is its ability to store and retrieve data in memory, which allows for extremely fast read and write operations. This makes Redis well-suited for use cases that require low-latency data access, such as real-time applications and caching. Redis supports a wide range of data structures, including strings, lists, sets, hashes, and sorted sets. This flexibility enables developers to model complex data scenarios and perform advanced operations on the data stored in Redis.\\n\\n## II. Creating a free Redis Instance\\nTo integrate Redis into your EdgeChains application, you\'ll first need to create a Redis instance. Follow these step-by-step instructions to create a Redis instance:\\n\\n**1. Sign up for Redis:** Visit [Redis website](https://redis.com/) and click on the _Try Free_ button in the top right corner. You can sign up using your preferred email address and password, or utilize your existing Google or GitHub account for a seamless registration process. \\n\\n![Redis landing page](./redis1.png)\\n\\nDuring the signup process, you can also select your desired cloud vendor and region and click on _Let\'s start free_ to continue. Once registered, you will be directed to the Redis dashboard.\\n\\n![Redis signup page](./redis2.png)\\n\\n**2. Access the Subscriptions Section:** In the Redis dashboard, navigate to the _Subscriptions_ section. Here, you will find information about your free subscription, which includes a storage limit of 30 MB.\\n\\n![Redis Subscription section](./sub.png)\\n\\n**3. Configure Your Free Subscription:**. Within the Subscriptions section, click on the configuration settings for your free subscription. This will provide you with the public endpoint of your Redis instance, which is crucial for establishing a connection.\\n\\n![Redis Configuration section](./config.png)\\n\\nCopy the public endpoint provided in the configuration settings. The endpoint typically looks like this:\\n\\n```bash\\nredis-19222.c1.us-east-1-3.ec2.cloud.redislabs.com:19222\\n```\\n\\n> In this example, redis-19222 represents the hostname of the Redis server, c1 is an identifier for a specific Redis instance or deployment, us-east-1-3.ec2.cloud represents the region or data center, and 19222 is the port number.\\n\\nTo establish a connection to your Redis instance, you need to extract the URL and port information from the endpoint. A Redis URL follows the format: \\n\\n```bash\\nredis://<host>:<port>\\n```\\n\\nwhere &lt;host&gt; represents the hostname or IP address of the Redis server, and&lt;port&gt represents the port number on which Redis is running. In the given example, redis-19222.c1.us-east-1-3.ec2.cloud.redislabs.com is the URL and 19222 is the port.\\n\\n> You will need to take note of the Redis URL and port to use them in your EdgeChains application.\\n\\nBy obtaining the Redis endpoint, you will have the necessary information to establish a connection to your Redis instance and start utilizing its features."},{"id":"github-gpt","metadata":{"permalink":"/blog/github-gpt","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-05-12-github-gpt/index.md","source":"@site/blog/2023-05-12-github-gpt/index.md","title":"How to Craft a Stellar GitHub Support Bot with GPT-3 and Chain-of-Thought","description":"Learn how to build an advanced GitHub support bot using GPT-3 and chain-of-thought techniques for improved user experience and efficient issue resolution.","date":"2023-05-12T00:00:00.000Z","formattedDate":"May 12, 2023","tags":[{"label":"chain-of-thought","permalink":"/blog/tags/chain-of-thought"},{"label":"llm","permalink":"/blog/tags/llm"},{"label":"github","permalink":"/blog/tags/github"},{"label":"arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":3.43,"hasTruncateMarker":false,"authors":[{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"github-gpt","title":"How to Craft a Stellar GitHub Support Bot with GPT-3 and Chain-of-Thought","description":"Learn how to build an advanced GitHub support bot using GPT-3 and chain-of-thought techniques for improved user experience and efficient issue resolution.","authors":{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["chain-of-thought","llm","github","arakoo"]},"prevItem":{"title":"How to create a new instance in Redis","permalink":"/blog/redis"},"nextItem":{"title":"Why you should be using chain-of-thought instead of prompts in chatGPT","permalink":"/blog/why-llm"}},"content":"## Introduction\\n\\nIn today\'s fast-paced software development world, efficient support and issue resolution is paramount to a project\'s success. Building a powerful GitHub support bot with GPT-3 and chain-of-thought techniques can help streamline the process and enhance user experience. This comprehensive guide will delve into the intricacies of creating such a bot, discussing the benefits, implementation, and performance optimization.\\n\\n### Benefits of a GitHub Support Bot\\n\\n1. **Faster issue resolution**: A well-designed support bot can quickly and accurately answer user queries or suggest appropriate steps to resolve issues, reducing the burden on human developers.\\n2. **Improved user experience**: A support bot can provide real-time assistance to users, ensuring a seamless and positive interaction with your project.\\n3. **Reduced workload for maintainers**: By handling repetitive and straightforward questions, the bot frees up maintainers to focus on more complex tasks and development work.\\n4. **Enhanced project reputation**: A responsive and knowledgeable support bot can boost your project\'s credibility and attract more contributors.\\n\\n### GPT-3: An Overview\\n\\n[OpenAI\'s GPT-3 (Generative Pre-trained Transformer 3)](https://arxiv.org/abs/2005.14165) is a state-of-the-art language model that can generate human-like text based on a given prompt. GPT-3 can be used for various tasks, such as question-answering, translation, summarization, and more. Its massive size (175 billion parameters) and pre-trained nature make it an ideal tool for crafting intelligent support bots.\\n\\n## Implementing a GitHub Support Bot with GPT-3\\n\\nTo build a GitHub support bot using GPT-3, follow these steps:\\n\\n### Step 1: Acquire API Access\\n\\nObtain access to the [OpenAI API](https://beta.openai.com/signup/) for GPT-3. Once you have API access, you can integrate it into your bot\'s backend.\\n\\n### Step 2: Set Up a GitHub Webhook\\n\\nCreate a [GitHub webhook](https://developer.github.com/webhooks/) to trigger your bot whenever an issue or comment is created. The webhook should be configured to send a POST request to your bot\'s backend with relevant data.\\n\\n### Step 3: Process Incoming Data\\n\\nIn your bot\'s backend, parse the incoming data from the webhook and extract the necessary information, such as issue title, description, and user comments.\\n\\n### Step 4: Generate Responses with GPT-3\\n\\nUsing the extracted information, construct a suitable prompt for GPT-3. Query the OpenAI API with this prompt to generate a response. Tools like [Arakoo EdgeChains](https://github.com/arakoodev/edgechains) help developers deal with the complexity of LLM & chain of thought.\\n\\n### Step 5: Post the Generated Response\\n\\nParse the response from GPT-3 and post it as a comment on the relevant issue using the [GitHub API](https://developer.github.com/v3/issues/comments/#create-a-comment).\\n\\n## Enhancing Support Bot Performance with Chain-of-Thought\\n\\nChain-of-thought is a technique that enables AI models to maintain context and coherence across multiple response generations. This section will discuss incorporating chain-of-thought into your GitHub support bot for improved performance.\\n\\n### Retaining Context in Conversations\\n\\nTo preserve context, store previous interactions (such as user comments and bot responses) in your bot\'s backend. When generating a new response, include the relevant conversation history in the GPT-3 prompt.\\n\\n### Implementing Multi-turn Dialogues\\n\\nFor complex issues requiring back-and-forth communication, implement multi-turn dialogues by continuously updating the conversation history and generating appropriate GPT-3 prompts.\\n\\n### Optimizing GPT-3 Parameters\\n\\nExperiment with GPT-3\'s API parameters, such as `temperature` and `top_p`, to control the randomness and quality of generated responses. Tools like Arakoo EdgeChains help developers deal with the complexity of LLM & chain of thought.\\n\\n## Monitoring and Improving Your Support Bot\'s Performance\\n\\nRegularly assess your bot\'s performance to ensure it meets user expectations and adheres to E-A-T (Expertise, Authoritativeness, Trustworthiness) and YMYL (Your Money or Your Life) guidelines.\\n\\n### Analyzing User Feedback\\n\\nMonitor user reactions and feedback to identify areas of improvement and optimize your bot\'s performance.\\n\\n### Refining GPT-3 Prompts\\n\\nIteratively improve your GPT-3 prompts based on performance analysis to generate more accurate and helpful responses.\\n\\n### Automating Performance Evaluation\\n\\nImplement automated performance evaluation metrics, such as response time and issue resolution rate, to gauge your bot\'s effectiveness.\\n\\n## Conclusion\\n\\nBuilding a GitHub support bot with GPT-3 and chain-of-thought techniques can significantly improve user experience and accelerate issue resolution. By following the steps outlined in this guide and continuously monitoring and optimizing performance, you can create a highly effective support bot that adds immense value to your project."},{"id":"why-llm","metadata":{"permalink":"/blog/why-llm","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-05-06-why-llm/index.md","source":"@site/blog/2023-05-06-why-llm/index.md","title":"Why you should be using chain-of-thought instead of prompts in chatGPT","description":"Chain of Thought","date":"2023-05-06T00:00:00.000Z","formattedDate":"May 6, 2023","tags":[{"label":"chain-of-thought","permalink":"/blog/tags/chain-of-thought"},{"label":"llm","permalink":"/blog/tags/llm"},{"label":"arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":4.17,"hasTruncateMarker":false,"authors":[{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"why-llm","title":"Why you should be using chain-of-thought instead of prompts in chatGPT","authors":{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["chain-of-thought","llm","arakoo"]},"prevItem":{"title":"How to Craft a Stellar GitHub Support Bot with GPT-3 and Chain-of-Thought","permalink":"/blog/github-gpt"},"nextItem":{"title":"how to get OpenAI API key","permalink":"/blog/openai-api-key"}},"content":"![Chain of Thought](./chain-of-thought.png)\\n\\n# Why You Should Be Using Chain-of-Thought Instead of Prompts in ChatGPT\\n\\n## Introduction\\nChatbot development has progressed considerably in recent years, with the advent of powerful algorithms like GPT-3. However, there exists a common problem where simple prompts do not suffice in effectively controlling the AI\'s output. Chain-of-thought, a more complex method for handling AI inputs, offers a better solution to this issue. In this article, we will dive deep into why chain-of-thought should play a significant role in your ChatGPT applications.\\n\\n## Benefits of Chain-of-Thought\\nWhile prompts might seem like a more straightforward approach, the advantages of using chain-of-thought in ChatGPT far outweigh their simplicity. By employing chain-of-thought, developers can enjoy various benefits that ultimately lead to improved capabilities in AI interactions.\\n\\n### Improved Controllability\\nOne of the most notable benefits of chain-of-thought is its ability to provide better controllability over AI-generated responses. Traditional prompt-based strategies often result in unexpected outputs that render the final outcomes unfit for their intended purpose. Chain-of-thought empowers developers to generate more precise responses, benefiting users in need of accurate and tailor-made outcomes.\\n\\n### Enhanced Flexibility\\nChain-of-thought allows developers to make adjustments and fine-tune their AI-generated responses in a more flexible manner. Unlike the prompt-based approach, which is burdened by its rigidity, chain-of-thought readily accommodates alterations in input parameters or context. This heightened adaptability makes it ideal for applications where the AI has to handle a broad range of evolving scenarios.\\n\\n### Greater Clarity and Context\\nIn many situations, prompts fail to provide sufficient information for generating coherent outputs. Chain-of-thought, on the other hand, emphasizes the importance of context, ensuring the AI fully understands the user\'s instructions. This results in more accurate and coherent responses, ultimately making communication with the AI more efficient and productive.\\n\\n### Better Conversational Flow\\nIn contrast to prompt-centric approaches, chain-of-thought excels at maintaining natural and engaging conversations. By incorporating an ongoing dialogue within the input, chain-of-thought helps ensure the AI\'s responses align seamlessly with the conversation\'s existing context. This promotes uninterrupted and more fluent exchanges between the AI and its users.\\n\\n### A Solution for Complex Applications\\nFor applications that demand a high degree of complexity, chain-of-thought serves as an invaluable tool in the developer\'s arsenal. Its emphasis on context, adaptability, and precision allows it to tackle demanding tasks that might otherwise prove unsuitable for simpler methods like prompts. Tools like Arakoo EdgeChains help developers deal with the complexity of LLM & chain of thought.\\n\\n## Implementing Chain-of-Thought in Your Applications\\nTo maximize the benefits of chain-of-thought in ChatGPT, it\'s essential to have a firm grasp of its key components and best practices for integration. By focusing on proper implementation and optimal usage, developers can unlock its full potential.\\n\\n### Methodological Considerations\\nChain-of-thought requires developers to shift their perspective from isolated prompts to a continuous stream of linked inputs. This necessitates a new approach to AI input formulation, where developers must construct sets of interconnected queries and statements in sequence, carefully ensuring each response is taken into consideration before constructing further inputs.\\n\\n### Effective Feedback Mechanisms\\nWith chain-of-thought, implementing an effective feedback mechanism is vital to improving the AI\'s understanding of the given context. Developers should leverage reinforcement learning approaches and constantly update their models with feedback gathered from users, progressively fine-tuning the AI to ensure higher quality outputs over time.\\n\\n### Tools and Technologies\\nTo facilitate chain-of-thought implementation, developers should familiarize themselves with relevant tools and technologies that simplify and streamline the process. Tools like Arakoo EdgeChains help developers deal with the complexity of LLM & chain of thought, while robust APIs and SDKs support the development of coherent input-output sequences for improved AI interactions.\\n\\n## Use Cases for Chain-of-Thought in ChatGPT\\nThe versatility of chain-of-thought has made it an increasingly popular choice for various applications across multiple industries, bolstering its reputation as an essential component of modern AI-powered solutions.\\n\\n### Customer Support\\nChain-of-thought can greatly enhance virtual customer support agents by providing them with the necessary context to handle diverse user queries accurately. This results in more personalized support experiences for users and increased efficiency for support teams.\\n\\n### Virtual Assistants\\nVirtual assistants can benefit from chain-of-thought by maintaining a continuous dialogue with users, making the interactions feel more natural and engaging. This ensures the AI maintains relevancy to the evolving user needs, thereby increasing its overall utility.\\n\\n### Interactive Gaming and Storytelling\\nThe dynamic nature of chain-of-thought makes it well-suited for complex applications in interactive gaming and storytelling. By allowing the virtual characters to respond intelligently based on the player\'s choices, it can cultivate more immersive and engaging experiences.\\n\\n## Conclusion\\nIn an era where AI applications are growing increasingly sophisticated, relying on traditional prompts is no longer sufficient. Chain-of-thought provides a more advanced and efficient approach to handling AI interactions, which, when implemented correctly, can lead to significant improvements in AI-generated outputs. By leveraging the power of chain-of-thought, developers can create transformative AI applications, ensuring their ChatGPT solutions remain at the cutting edge of innovation."},{"id":"openai-api-key","metadata":{"permalink":"/blog/openai-api-key","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-05-05-openai-api-key/index.md","source":"@site/blog/2023-05-05-openai-api-key/index.md","title":"how to get OpenAI API key","description":"OpenAI logo","date":"2023-05-05T00:00:00.000Z","formattedDate":"May 5, 2023","tags":[{"label":"openai","permalink":"/blog/tags/openai"},{"label":"arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":6.995,"hasTruncateMarker":false,"authors":[{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"openai-api-key","title":"how to get OpenAI API key","authors":{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["openai","arakoo"]},"prevItem":{"title":"Why you should be using chain-of-thought instead of prompts in chatGPT","permalink":"/blog/why-llm"},"nextItem":{"title":"Vector-Databas-at-a-Glance","permalink":"/blog/Vector-Database"}},"content":"![OpenAI logo](./openai.png)\\n\\n# Introduction\\n\\nIntegrating AI services into your projects has become increasingly important, and obtaining an OpenAI API key is a vital step in this process. By acquiring an API key, you unlock access to OpenAI\'s robust natural language processing capabilities, empowering you to optimize the efficiency and precision of your applications. In this comprehensive guide, we will walk you through the step-by-step process of obtaining an OpenAI API key. Tools like [Arakoo EdgeChains](https://github.com/arakoodev/edgechains) can greatly assist you in utilizing the OpenAI API seamlessly.\\n\\n## Is an OpenAI API Key Free?\\n\\n**Free Trial, Credit and Billing Information**\\n\\nYou can create an OpenAI API key free of charge. As a new user, you will receive $5 (USD) worth of credit as part of the free trial. However, please note that this credit expires after three months. Once your credit has been utilized or expired, you have the option to enter your billing information to continue using the API according to your requirements. It\'s important to remember that if you do not provide billing information, you will still have login access but won\'t be able to make additional API requests.\\n\\n**Rate Limits:**\\n\\nOpenAI implements rate limits at the organizational level, and if you are using their services for business purposes, payment may be required based on certain factors. Rate limits are measured in two ways: RPM (requests per minute) and TPM (tokens per minute).\\n\\n**Cost and Pricing:**\\n\\nIf you are interested in specific costs associated with the AI model you intend to use (e.g., GPT-4 or gpt-3.5-turbo, as employed in ChatGPT), you can refer to [OpenAI\'s AI model pricing page](https://openai.com/pricing). In many cases, utilizing the API could be more cost-effective than a paid ChatGPT Plus subscription, although the actual expenses depend on your usage. \\n\\n> For a comprehensive overview of precise rate limits, examples, and other valuable details, we recommend visiting [OpenAI\'s Rate Limits page](https://platform.openai.com/docs/guides/rate-limits).\\n\\n\\n## How do I get an OpenAI API Key?\\nTo begin with, follow the steps below.\\n\\n### 1. Create an OpenAI account\\n\\nTo get started, please navigate to the [OpenAI platform website](https://openai.com/) and proceed with creating an account by following the provided instructions. You have the option to sign up using your preferred email address and password, or alternatively, you can utilize your existing Google or Microsoft account for a seamless registration process.\\n\\n![OpenAI login page](./Login.png)\\n\\nAfter completing the registration, OpenAI will send you a confirmation email to verify your account. Please locate the email in your inbox and click on the verification link provided to ensure the utmost security of your account. Once you have verified your account, return to the OpenAI website and click on the \\"Log In\\" button. \\n\\n### 2. Navigate to the API section\\n\\nUpon logging in, you will locate your name and profile icon situated in the upper-right corner of the OpenAI platform homepage. Please click on your name to unveil a dropdown menu, and proceed to select the _View API keys_ option. \\n\\nAlternatively you can navigate to the [apps section](https://platform.openai.com/apps) and click on API. \\n\\n![OpenAI API page](./APIpage.png)\\n\\n### 3. Generate a new API key\\nIn the API keys section, find the _Create new secret key_ button and proceed to click on it in order to generate a fresh API key. A dialog box will promptly appear, requesting you to provide a descriptive name for your secret API key. It is advisable to choose a name that conveys its purpose clearly, facilitating future identification. \\n\\n![OpenAI API Key page](./create.jpeg)\\n\\nEnsure that you save the API key promptly, as the window displaying it cannot be reopened once closed.\\n\\n![OpenAI API Key page](./create2.png)\\n### 4. Set up billing\\nOpenAI charges for API usage based on your usage volume. Therefore, if you haven\'t already set up a payment method for billing, it\'s necessary to do so before your newly created API key can function.\\n\\nTo initiate the billing setup process, navigate to the _Billing_ section located in the left-hand menu, followed by selecting the _Payment methods_ option. \\n\\n![OpenAI Billing page](./pay.png)\\n\\nWithin the payment methods interface, you will find an option labeled _Set up paid account_. This option allows you to choose between two methods: For Individual and Company.\\n\\n![OpenAI Billing page](./pay2.png)\\n\\nBy clicking on any option, a pop-up window will emerge, facilitating the input of your credit card details and pertinent billing information. Once you have provided all the required information, please proceed by clicking _Submit_ to finalize the process.\\n\\n![OpenAI Billing page](./pay3.png)\\n\\n### 5. Set usage limits\\nTo ensure efficient management of your monthly API expenditure, it is advisable to establish usage limits after setting up the billing process.\\n\\nTo proceed, navigate to the left menu and select the option _Usage limits_. Here, you can define both hard and soft usage limits based on your specific requirements. Once you have determined the desired limits, simply click on the _Save_ button to save your changes.\\n\\nBy following these steps, you will successfully obtain an OpenAI API key and be ready to harness the power of OpenAI\'s natural language processing capabilities in your projects.\\n\\n![OpenAI Usage page](./usage.png)\\n\\n### Ensure you follow OpenAI\'s usage guidelines\u200d\\n\\nAs a final note, be sure to familiarize yourself with OpenAI\'s use case policy and terms of use.You can find detailed information regarding these policies at the [OpenAI usage policies](https://openai.com/policies/usage-policies) .\\n\\n## API Key Best Practices\\n\\n### Key Security\\n\\nWhen it comes to securing your OpenAI API key, it is crucial to follow best practices to protect sensitive information. Here are some key security measures to consider:\\n1. **Secure Storage**: Store your API key in a secure location, such as a password-protected and encrypted storage system. Avoid storing it in plain text or easily accessible locations, such as code repositories.\\n2. **Restricted Access**: Limit access to your API key to authorized individuals only. Implement robust access controls and authentication mechanisms to ensure that only trusted parties can retrieve and use the key.\\n3. **Device Limitations**: Minimize the number of devices that store your API key. By reducing the number of endpoints where the key is stored, you can reduce the potential attack surface and enhance overall security.\\n\\n### Key Rotation\\nRegularly rotating your API key is essential for maintaining its security. By frequently changing your key, you mitigate the risks associated with long-term exposure or compromise. Follow these guidelines for effective key rotation:\\n1. **Timely Updates**: Whenever you change your API key, make sure to promptly update it across all integrations and applications that rely on it. This ensures that any potential vulnerabilities associated with the previous key are eliminated.\\n2. **Automation Tools**: Consider leveraging automation tools specifically designed for managing and rotating multiple API keys. One such tool is [Arakoo EdgeChains](https://www.arakoo.ai/), which provides seamless key management capabilities to simplify the process.\\n\\n## Integrating the OpenAI API\\n\\n### Selecting the Appropriate API Endpoint\\nDepending on your use case, you may need to interact with different API endpoints provided by OpenAI, such as `Completion` or `Translation`. Ensure to review OpenAI\'s documentation to understand which endpoint best fits your needs.\\n\\n### API Request and Response Handling\\nWhen integrating the OpenAI API, be sure to handle requests and responses properly. Construct the appropriate request headers and payloads based on OpenAI\'s documentation, and handle potential errors gracefully. Implement appropriate timeouts and error-handling mechanisms to maintain the stability of your application.\\n\\n## Common Use Cases for the OpenAI API\\n\\n### AI-Generated Content\\nThe `Completion` endpoint enables the generation of human-like, context-relevant content for a variety of purposes, such as article drafting, email composition, and social media posting.\\n\\n### Natural Language Translation\\nWith OpenAI\'s `Translation` endpoint, you can easily translate text between various languages, assisting with communication in multilingual environments.\\n\\n### Sentiment Analysis\\nBy analyzing the emotion or tone of content, OpenAI can provide valuable insights for customer relationship management or market research.\\n\\n### Text Summarization\\nThe API can help in condensing long documents, articles, or emails into brief, coherent summaries, saving valuable time and improving readability.\\n\\n### Question and Answer Systems\\nLeveraging OpenAI\'s natural language understanding, creating intelligent chatbots and automated customer support systems is simplified.\\n\\n# Conclusion\\nAcquiring an OpenAI API key will unlock the potential of OpenAI\'s powerful language processing capabilities for your projects. Following best practices and carefully integrating the API into your projects will help you make the most of these powerful tools. Remember, tools like [Arakoo EdgeChains](https://github.com/arakoodev/edgechains) can assist you in the integration process, enabling seamless use of the OpenAI API."},{"id":"Vector-Database","metadata":{"permalink":"/blog/Vector-Database","editUrl":"https://github.com/arakoodev/doc/tree/main/blog/2023-02-28-vector-database/index.md","source":"@site/blog/2023-02-28-vector-database/index.md","title":"Vector-Databas-at-a-Glance","description":"Vector Database","date":"2023-02-28T00:00:00.000Z","formattedDate":"February 28, 2023","tags":[{"label":"Vector Database","permalink":"/blog/tags/vector-database"},{"label":"Arakoo","permalink":"/blog/tags/arakoo"}],"readingTime":5.765,"hasTruncateMarker":false,"authors":[{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"}],"frontMatter":{"slug":"Vector-Database","title":"Vector-Databas-at-a-Glance","authors":{"name":"Arakoo","title":"Arakoo Core Team","url":"https://github.com/arakoodev","image_url":"https://avatars.githubusercontent.com/u/114422989","imageURL":"https://avatars.githubusercontent.com/u/114422989"},"tags":["Vector Database","Arakoo"]},"prevItem":{"title":"how to get OpenAI API key","permalink":"/blog/openai-api-key"}},"content":"![Vector Database](./Screenshot 2023-08-28 at 5.22.13 PM.png)\\n## Vector Database At A Glance\\n\\n## Introduction \u2013 \\nA vector database is a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. Each vector has a certain number of dimensions, which can range from tens to thousands, depending on the complexity and granularity of the data. The vectors are usually generated by applying some kind of transformation or embedding function to the raw data, such as text, images, audio, video, and others. The embedding function can be based on various methods, such as machine learning models, word embeddings, feature extraction algorithms. Edegchian uses vector database and enhances the backend capabilities of applications built with the framework, enabling developers to create advanced and interactive applications powered by large language models.\\n\\n## Vector Database? \\nInformation comes in many forms. Some information is unstructured\u2014like text documents, rich media, and audio\u2014and some is structured\u2014like application logs, tables, and graphs. Innovations in artificial intelligence and machine learning (AI/ML) have allowed us to create a type of ML model\u2014embedding models. Embeddings encode all types of data into vectors that capture the meaning and context of an asset. This allows us to find similar assets by searching for neighboring data points. Vector search methods allow unique experiences like taking a photograph with your smartphone and searching for similar images. \\nVector databases provide the ability to store and retrieve vectors as high-dimensional points. They add additional capabilities for efficient and fast lookup of nearest-neighbors in the N-dimensional space. They are typically powered by k-nearest neighbor (k-NN) indexes and built with algorithms like the Hierarchical Navigable Small World (HNSW) and Inverted File Index (IVF) algorithms. Vector databases provide additional capabilities like data management, fault tolerance, authentication and access control, and a query engine. \\nFor example, you can use a vector database to:\\n\u2022\\tfind images that are similar to a given image based on their visual content and style\\n\u2022\\tfind documents that are similar to a given document based on their topic and sentiment\\n\u2022\\tfind products that are similar to a given product based on their features and ratings\\nTo perform similarity search and retrieval in a vector database, you need to use a query vector that represents your desired information or criteria. The query vector can be either derived from the same type of data as the stored vectors (e.g., using an image as a query for an image database), or from different types of data (e.g., using text as a query for an image database). Then, you need to use a similarity measure that calculates how close or distant two vectors are in the vector space. The similarity measure can be based on various metrics, such as cosine similarity, euclidean distance, hamming distance, jaccard index.\\nThe result of the similarity search and retrieval is usually a ranked list of vectors that have the highest similarity scores with the query vector. You can then access the corresponding raw data associated with each vector from the original source or index.\\n\\n## Importance of Vector Database\\nYour developers can index vectors generated by embeddings into a vector database. This allows allowing them to find similar assets by querying for neighboring vectors.\\nVector databases provide a method to operationalize embedding models. Application development is more productive with database capabilities like resource management, security controls, scalability, fault tolerance, and efficient information retrieval through sophisticated query languages.\\nVector databases ultimately empower developers to create unique application experiences. For example, your users could snap photographs on their smartphones to search for similar images. \\nDevelopers can use other types of machine learning models to automate metadata extraction from content like images and scanned documents. They can index metadata alongside vectors to enable hybrid search on both keywords and vectors. They can also fuse semantic understanding into relevancy ranking to improve search results.\\nInnovations in generative artificial intelligence (AI) have introduce new types of models like ChatGPT that can generate text and manage complex conversations with humans. Some can operate on multiple modalities; for instance, some models allow users to describe a landscape and generate an image that fits the description.\\nGenerative models are, however, prone to hallucinations, which could, for instance, cause a chatbot to mislead users. Vector databases can complement generative AI models. They can provide an external knowledge base for generative AI chatbots and help ensure they provide trustworthy information. \\nHow are vector databases used?\\nVector databases are typically used to power vector search use cases like visual, semantic, and multimodal search. More recently, they\u2019re paired with generative artificial intelligence (AI) text models to create intelligent agents that provide conversational search experiences.\\nThe development process starts with building an embedding model that\u2019s designed to encode a corpus like product images into vectors. The data import process is also called data hydration. The application developer can now use the database to search for similar products by encoding a product image and using the vector to query for similar images.\\nWithin the model, the k-nearest neighbor (k-NN) indexes provide efficient retrieval of vectors and apply a distance function like cosine to rank results by similarity. \\n\\n## Use-Case Of Vector Database\\nVector databases are for developers who want to create vector search powered experiences. An application developer can use open-source models, automated machine learning (ML) tools, and foundational model services to generate embeddings and hydrate a vector database. This requires minimal ML expertise.\\nA team of data scientists and engineers can build expertly tuned embeddings and operationalize them through a vector database. This can help them deliver artificial intelligence (AI) solution faster.\\nOperations teams benefit from managing solutions as familiar database workloads. They can use existing tools and playbooks.\\nWhat are the benefits of vector databases?\\nVector databases allow developers to innovate and create unique experiences powered by vector search. They can accelerate artificial intelligence (AI) application development and simplify the operationalization of AI-powered application workloads.\\nVector databases provide an alternative to building on top of bare k-nearest neighbor (k-NN) indexes. That kind of index requires a great deal of additional expertise and engineering to use, tune and operationalize.\\nA good vector database provides applications with a foundation through features like data management, fault tolerance, critical security features, and a query engine. These capabilities allow users to operationalize their workloads to simplify scaling, maintain high scalability, and support security requirements.\\nCapabilities like the query engine and SDKs simplify application development. They also allow developers to perform more advanced queries (like searching and filtering) on metadata as part of a k-NN search. They also have the option to use hybrid relevancy scoring models that blend traditional term frequency models like BM25 with vector scores to enhance information retrieval.\\n\\n## Edgechain and Vector Database-\\nThe following API Examples will give you brief description of how edgechain uses vector databases -\\n- [ Creating HistoryContext (Using Redis) Controller](#6-creating-historycontext-using-redis-controller)\\n- [ Creating HistoryContext (Using PostgreSQL) Controller](#7-creating-historycontext-using-postgresql-controller)"}]}')}}]);