"use strict";(self.webpackChunkalekhaweb=self.webpackChunkalekhaweb||[]).push([[7685],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>g});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},a=Object.keys(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)n=a[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=r.createContext({}),l=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},u=function(e){var t=l(e.components);return r.createElement(s.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,a=e.originalType,s=e.parentName,u=p(e,["components","mdxType","originalType","parentName"]),c=l(n),m=o,g=c["".concat(s,".").concat(m)]||c[m]||d[m]||a;return n?r.createElement(g,i(i({ref:t},u),{},{components:n})):r.createElement(g,i({ref:t},u))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var a=n.length,i=new Array(a);i[0]=m;var p={};for(var s in t)hasOwnProperty.call(t,s)&&(p[s]=t[s]);p.originalType=e,p[c]="string"==typeof e?e:o,i[1]=p;for(var l=2;l<a;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},5639:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>p,toc:()=>l});var r=n(7462),o=(n(7294),n(3905));const a={},i="Prompts: Managing LLM inputs",p={unversionedId:"Edgechain Modules/Prompts: Managing LLM inputs",id:"Edgechain Modules/Prompts: Managing LLM inputs",title:"Prompts: Managing LLM inputs",description:"LLMs have weird APIs. Although inputting prompts to LLMs in natural language should feel intuitive, it takes quite a bit of tweaking of the prompt until you get the desired output from an LLM. This process is called prompt engineering.",source:"@site/doc/Edgechain Modules/Prompts: Managing LLM inputs.md",sourceDirName:"Edgechain Modules",slug:"/Edgechain Modules/Prompts: Managing LLM inputs",permalink:"/doc/Edgechain Modules/Prompts: Managing LLM inputs",draft:!1,editUrl:"https://github.com/arakoodev/doc/tree/main/doc/Edgechain Modules/Prompts: Managing LLM inputs.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Models: Choosing from different LLMs and embedding models",permalink:"/doc/Edgechain Modules/Module 1: Choosing from different LLMs and embedding models"}},s={},l=[],u={toc:l},c="wrapper";function d(e){let{components:t,...n}=e;return(0,o.kt)(c,(0,r.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"prompts-managing-llm-inputs"},"Prompts: Managing LLM inputs"),(0,o.kt)("p",null,"LLMs have weird APIs. Although inputting prompts to LLMs in natural language should feel intuitive, it takes quite a bit of tweaking of the prompt until you get the desired output from an LLM. This process is called prompt engineering.\nOnce you have a good prompt, you may want to use it as a template for other purposes. Thus, Edgechain provides you with so-called PromptTemplates, which help you construct prompts from multiple components."),(0,o.kt)("p",null,"With the help of Edgechains, we can utilise Wikipedia as a prompt for the OpenAiChatCompletion API."),(0,o.kt)("p",null,"  Creating the Prompt for OpenAiChatCompletion API"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'String query = arkRequest.getQueryParam("query");\nboolean stream = arkRequest.getBooleanHeader("stream");\n\nreturn new EdgeChain<>(wikiEndpoint.getPageContent(query))\n    .transform(wiki -> {\n        loader\n            .put("keepContext", new JsonnetArgs(DataType.BOOLEAN, "true"))\n            .put("context", new JsonnetArgs(DataType.STRING, wiki.getText()))\n            .loadOrReload();\n        return loader.get("prompt");\n    })\n    .transform(openAiEndpoint::getChatCompletion)\n    .getArkResponse();\n')),(0,o.kt)("p",null,"With this, you will be able to fetch content from Wikipedia, create the prompt using the JSONnet file, and then pass it to the OpenAiChatCompletion API. This will enable you to utilize the powerful capabilities of OpenAI's language models for processing Wikipedia content and generating the desired output."),(0,o.kt)("p",null,"For further details on the same example check out-\n",(0,o.kt)("a",{parentName:"p",href:"https://www.arakoo.com/doc/API%20documentation/Examples"},"Getting Wikipedia Content")))}d.isMDXComponent=!0}}]);